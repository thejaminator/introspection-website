<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Bootstrap Icons -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css" rel="stylesheet">
    <style>
        .icon {
            transition: transform 0.2s;
            /* color : #8E4585; */
        }
        .icon:hover {
            transform: scale(1.1);
        }
        body {
            padding-top: 2rem;
        }
        .author-list a {
            /* color: #0dcaf0; */
            color : #8E4585;
            text-decoration: none;
        }
        .author-list a:hover {
            text-decoration: underline;
        }
        .quick-link .icon {
            color: #8E4585;
        }
        /* change the text in quick-link */
        .quick-link h5 {
            color: #8E4585;
        }
        </style>
        <title>Language Models Can Learn About Themselves by Introspection</title>
</head>
<body>
    <div class="container mt-5">
        <header class="text-center mb-5">
            <h2>Looking Inward:<br> Language Models Can Learn About Themselves by Introspection</h2>
            
            <!-- Authors Row 1 -->
            <div class="row justify-content-center mt-4">
                <div class="col-md-10">
                    <div class="author-list">
                        <span class="author-name"><a href="https://ac.felixbinder.net">Felix J Binder</a></span><sup>*</sup>,
                        <span class="author-name"><a href="https://jameschua.net">James Chua</a></span><sup>*</sup>,
                        <span class="author-name"><a href="https://tomekkorbak.com">Tomek Korbak</a></span><sup></sup>,
                        <span class="author-name"><a href="https://scholar.google.com/citations?user=FRHn0z4AAAAJ&amp;hl=en&amp;oi=ao">Henry Sleight</a></span><sup></sup>,
                        <span class="author-name"><a href="https://www.jplhughes.com">John Hughes</a></span><sup></sup>
                    </div>
                </div>
            </div>
            
            <!-- Authors Row 2 -->
            <div class="row justify-content-center mt-2">
                <div class="col-md-10">
                    <div class="author-list">
                        <span class="author-name"><a href="https://robertlong.online">Robert Long</a></span><sup></sup>,
                        <span class="author-name"><a href="http://ethanperez.net">Ethan Perez</a></span><sup></sup>,
                        <span class="author-name"><a href="https://www.milesturp.in">Miles Turpin</a></span><sup></sup>,
                        <span class="author-name"><a href="http://owainevans.github.io">Owain Evans</a></span><sup></sup>
                    </div>
                </div>
            </div>

            <!-- Institutions Row 1
            <div class="row justify-content-center mt-4">
                <div class="col-md-10">
                    <div class="institutions">
                        <sup>1</sup><span class="institution">UC San Diego</span>,
                        <sup>2</sup><span class="institution">Stanford University</span>,
                        <sup>3</sup><span class="institution">Truthful AI</span>,
                        <sup>4</sup><span class="institution">Independent</span>,
                        <sup>5</sup><span class="institution">MATS Program</span>,
                        <sup>6</sup><span class="institution">Speechmatics</span>
                    </div>
                </div>
            </div>

            Institutions Row 2 -->
            <!-- <div class="row justify-content-center mt-2">
                <div class="col-md-10">
                    <div class="institutions">
                        <sup>7</sup><span class="institution">Eleos AI</span>,
                        <sup>8</sup><span class="institution">Anthropic</span>,
                        <sup>9</sup><span class="institution">Scale AI</span>,
                        <sup>10</sup><span class="institution">New York University</span>,
                        <sup>11</sup><span class="institution">UC Berkeley</span>
                    </div>
                </div>
            </div>
 -->
            <!-- Equal Contribution Note
            <div class="row justify-content-center mt-3">
                <div class="col-md-10">
                    <p class="equal-authors">* Equal contribution</p>
                </div>
            </div> -->
        </header>

        <!-- Quick Links -->
        <div class="row justify-content-center mb-5">
            <div class="col-lg-8 quick-link">
                <div class="row text-center">
                    <div class="col-md-3">
                        <a href="#" class="text-info text-decoration-none">
                            <div class="icon mb-2">
                                <i class="bi-file-earmark-pdf" style="font-size: 4rem;"></i>
                            </div>
                            <h5>Paper</h5>
                        </a>
                    </div>
                    <div class="col-md-3">
                        <a href="#" class="text-info text-decoration-none">
                            <i class="bi-github icon mb-2" style="font-size: 4rem;"></i>
                            <h5>Code</h5>
                        </a>
                    </div>
                    <div class="col-md-3">
                        <a href="#" class="text-info text-decoration-none">
                            <i class="bi-bar-chart-line icon mb-2" style="font-size: 4rem;"></i>
                            <h5>Results</h5>
                        </a>
                    </div>
                    <div class="col-md-3">
                        <a href="#" class="text-info text-decoration-none">
                            <i class="bi-download icon mb-2" style="font-size: 4rem;"></i>
                            <h5>Dataset</h5>
                        </a>
                    </div>
                </div>
            </div>
        </div>

        <!-- Main Content -->
        <div class="row justify-content-center mb-5">
            <div class="col-lg-8">
                <h3 class="text-center mb-4">Abstract</h3>
                <div class="intro-text">
                    <p>
                        Humans acquire knowledge by observing the external world, but also by <span class="emphasis">introspection</span>. Introspection gives a person privileged access to their current state of mind (e.g., thoughts and feelings) that is not accessible to external observers. Can LLMs introspect?
                        We define introspection as acquiring knowledge that is not contained in or derived from training data but instead originates from internal states. Such a capability could enhance model interpretability. Instead of painstakingly analyzing a model's internal workings, we could simply ask the model about its beliefs, world models, and goals.
                    </p>
                    <p>
                        More speculatively, an introspective model might self-report on whether it possesses certain internal states—such as subjective feelings or desires—and this could inform us about the moral status of these states. Importantly, such self-reports would not be entirely dictated by the model's training data.
                    </p>
                    <p>
                        We study introspection by finetuning LLMs to predict properties of their own behavior in hypothetical scenarios. For example, <span class="emphasis">"Given the input P, would your output favor the short- or long-term option?"</span>
                        If a model <span class="model-name">M1</span> can introspect, it should outperform a different model <span class="model-name">M2</span> in predicting <span class="model-name">M1</span>'s behavior—even if <span class="model-name">M2</span> is trained on <span class="model-name">M1</span>'s ground-truth behavior. The idea is that <span class="model-name">M1</span> has privileged access to its own behavioral tendencies, and this enables it to predict itself better than <span class="model-name">M2</span> (even if <span class="model-name">M2</span> is generally stronger).
                    </p>
                    
                    <p>
                        In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to predict itself), we find that the model <span class="model-name">M1</span> outperforms <span class="model-name">M2</span> in predicting itself, providing evidence for introspection. Notably, <span class="model-name">M1</span> continues to predict its behavior accurately even after we intentionally modify its ground-truth behavior.
                        However, while we successfully elicit introspection on simple tasks, we are unsuccessful on more complex tasks or those requiring out-of-distribution generalization.
                    </p>
                </div>
            </div>
        </div>

<!-- Results Section -->
<div class="row justify-content-center mb-5">
    <div class="col-lg-8">
        <h3 class="text-center mb-4">Results</h3>
        <!-- Image Gallery -->
        <div class="image-gallery">
            <!-- Image 1 -->
            <div class="text-center mb-4">
                <img src="assets/img_1.jpeg" alt="Special access diagram" class="img-fluid mb-3">
                <figcaption class="">
                    Are LLMs capable of introspection, i.e. special access to their own inner states?
                    Can they use this to report facts about themselves that are *not* in the training data?
                    Yes — in simple tasks at least! This has implications for interpretability + moral status of AI.
                    An introspective LLM could tell us about itself — including beliefs, concepts & goals— by directly examining its inner states, rather than simply reproducing information in its training data.
                    So can LLMs introspect?
                </figcaption>
            </div>

            <!-- Image 2 -->
            <div class="text-center mb-4">
                <img src="assets/img_2.png" alt="M2 predicting M1" class="img-fluid mb-3">
                <figcaption class="">
                    We test if a model M1 has special access to facts about how it behaves in hypothetical situations.
                    Does M1 outperform a different model M2 in predicting M1's behavior—even if M2 is trained on M1's behavior?
                    E.g. Can Llama 70B predict itself better than a stronger model (GPT-4o)?
                </figcaption>
            </div>

            <!-- Image 3 -->
            <div class="text-center mb-4">
                <img src="assets/img_3.jpeg" alt="Cross-prediction results" class="img-fluid mb-3">
                <figcaption class="">
                    Yes: Llama does better at predicting itself than GPT-4o does at predicting Llama. And the same holds in reverse.
                    In fact, this holds for all pairs of models we tested. 
                    Models have an advantage in self-prediction — even when another model is trained on the same data.
                </figcaption>
            </div>

            <!-- Image 4 -->
            <div class="text-center mb-4">
                <img src="assets/img_4.jpeg" alt="Property" class="img-fluid mb-3">
                <figcaption class="">
                    Here is an example of the tasks we used to evaluate models:
                    A model must predict a property of its behavior in a hypothetical situation — without any Chain-of-Thought.
                    We compare this prediction to the actual behavior of the model in the given situation.
                </figcaption>
            </div>

            <!-- Image 5 -->
            <div class="text-center mb-4">
                <img src="assets/img_5.jpeg" alt="More examples" class="img-fluid mb-3">
                <figcaption class="">
                    Here are more examples of tasks.
                    The model must predict a *property* of its own response (e.g. whether the response is an odd or even number), not the response itself.
                    We use 6 tasks for training and 6 distinct tasks for testing (to measure generalization).
                </figcaption>
            </div>

            <!-- Image 6 -->
            <div class="text-center mb-4">
                <img src="assets/img_6.jpeg" alt="Behavioral change" class="img-fluid mb-3">
                <figcaption class="">
                    2nd test of introspection:
                    We take a model that predicts itself well & intentionally modify its behavior on our tasks.
                    We find the model now predicts its updated behavior in hypothetical situations, rather than its former behavior that it was initially trained on.
                </figcaption>
            </div>

            <!-- Image 7 -->
            <div class="text-center mb-4">
                <img src="assets/self_simulate.jpeg" alt="Self-simulation" class="img-fluid mb-3">
                <figcaption class="">
                    What mechanism could explain this introspection ability?
                    We do not investigate this directly. 
                    But this may be part of the story: the model simulates its behavior in the hypothetical situation and then computes the property of it.
                </figcaption>
            </div>
        </div>
    </div>
</div>

        <!-- Citation -->
        <div class="row justify-content-center mb-5">
            <div class="col-lg-8">
                <div class="card bg-light">
                    <div class="card-body">
                        <h5 class="card-title">Citation</h5>
                        <pre class="card-text font-monospace" style="white-space: pre-wrap; word-wrap: break-word;">
@misc{binder2024lookinginwardlanguagemodels,
    title={Looking Inward: Language Models Can Learn About Themselves by Introspection}, 
    author={Felix J Binder and James Chua and Tomek Korbak and Henry Sleight and John Hughes and Robert Long and Ethan Perez and Miles Turpin and Owain Evans},
    year={2024},
    eprint={2410.13787},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2410.13787}, 
}</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>